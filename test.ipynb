{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f8578a-9b61-4fa4-afd4-e69675320da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainly from https://github.com/borutb-fri/FMLD/blob/main/mask-test.py\n",
    "# Download model from:\n",
    "# https://unilj-my.sharepoint.com/:u:/g/personal/borutb_fri1_uni-lj_si/EdmDsIgG9XBJkRVXDKyOwvEBK7pK1EEq9cBfOVm3kLzPvw?e=M9pULa\n",
    "# model also from https://github.com/borutb-fri/FMLD/\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c39d067e-1355-4b1c-97e9-13c7536dab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(root):\n",
    "    # Applying Transforms to the Data\n",
    "    image_transform = transforms.Compose([\n",
    "            transforms.Resize(size=(224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    directories = {\n",
    "        type_: os.path.join(root, type_) for type_ in next(os.walk(root))[1]\n",
    "    }\n",
    "    types = directories.keys()\n",
    "\n",
    "    # Batch size\n",
    "    bs = 128\n",
    "\n",
    "    # Number of classes\n",
    "    num_classes = 2\n",
    "crop\n",
    "    # Load Data from folders\n",
    "    ds = {\n",
    "        type_: datasets.ImageFolder(root=directories[type_], transform=image_transform)\n",
    "                                    for type_ in  directories\n",
    "    }\n",
    "    \n",
    "    dls = {type_: torch.utils.data.DataLoader(ds[type_], batch_size=bs, shuffle=True, num_workers=4)\n",
    "              for type_ in types\n",
    "    }\n",
    "    dls = {type_: {\"dl\": dls[type_], \"size\": len(ds[type_])} for type_ in types}\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f041d44-65f8-4630-9094-71e72b19ab7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'dl': <torch.utils.data.dataloader.DataLoader at 0x7fa152b470d0>,\n",
       "  'size': 42761},\n",
       " 'test': {'dl': <torch.utils.data.dataloader.DataLoader at 0x7fa152b47fd0>,\n",
       "  'size': 10481}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders = get_ds('__FULL'); data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a16f3f1c-92b5-48ef-b8c2-ba365d31a58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces:  10481\n",
      "tensor(0.9062)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.8906)\n",
      "tensor(0.9062)\n",
      "tensor(0.9141)\n",
      "tensor(0.8828)\n",
      "tensor(0.9375)\n",
      "tensor(0.9453)\n",
      "tensor(0.9141)\n",
      "tensor(0.9609)\n",
      "tensor(0.9531)\n",
      "tensor(0.9141)\n",
      "tensor(0.8594)\n",
      "tensor(0.8906)\n",
      "tensor(0.9141)\n",
      "tensor(0.9453)\n",
      "tensor(0.9219)\n",
      "tensor(0.8906)\n",
      "tensor(0.8984)\n",
      "tensor(0.8984)\n",
      "tensor(0.9141)\n",
      "tensor(0.9219)\n",
      "tensor(0.9375)\n",
      "tensor(0.9453)\n",
      "tensor(0.8906)\n",
      "tensor(0.9375)\n",
      "tensor(0.9141)\n",
      "tensor(0.8984)\n",
      "tensor(0.8906)\n",
      "tensor(0.9062)\n",
      "tensor(0.8984)\n",
      "tensor(0.9297)\n",
      "tensor(0.9297)\n",
      "tensor(0.8984)\n",
      "tensor(0.9453)\n",
      "tensor(0.9375)\n",
      "tensor(0.9141)\n",
      "tensor(0.9219)\n",
      "tensor(0.9297)\n",
      "tensor(0.9062)\n",
      "tensor(0.8906)\n",
      "tensor(0.9297)\n",
      "tensor(0.9375)\n",
      "tensor(0.8828)\n",
      "tensor(0.9297)\n",
      "tensor(0.9141)\n",
      "tensor(0.9453)\n",
      "tensor(0.9062)\n",
      "tensor(0.9453)\n",
      "tensor(0.9609)\n",
      "tensor(0.9297)\n",
      "tensor(0.9375)\n",
      "tensor(0.8906)\n",
      "tensor(0.8906)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.8672)\n",
      "tensor(0.9219)\n",
      "tensor(0.9297)\n",
      "tensor(0.9062)\n",
      "tensor(0.8750)\n",
      "tensor(0.9141)\n",
      "tensor(0.9297)\n",
      "tensor(0.9062)\n",
      "tensor(0.9062)\n",
      "tensor(0.9531)\n",
      "tensor(0.9375)\n",
      "tensor(0.8750)\n",
      "tensor(0.9297)\n",
      "tensor(0.9297)\n",
      "tensor(0.8828)\n",
      "tensor(0.8828)\n",
      "tensor(0.8828)\n",
      "tensor(0.9062)\n",
      "tensor(0.9062)\n",
      "tensor(0.9297)\n",
      "tensor(0.9141)\n",
      "tensor(0.9375)\n",
      "tensor(0.9453)\n",
      "tensor(0.9219)\n",
      "tensor(0.9469)\n",
      "Test accuracy : 0.9161339564446718\n"
     ]
    }
   ],
   "source": [
    "# Print the test set data sizes\n",
    "data_size = data_loaders[\"test\"][\"size\"]\n",
    "data_loader = data_loaders[\"test\"][\"dl\"]\n",
    "\n",
    "print('Number of faces: ',data_loaders[\"test\"][\"size\"])\n",
    "\n",
    "def computeTestSetAccuracy(model, loss_criterion, data_loader, data_size):\n",
    "    '''\n",
    "    Function to compute the accuracy on the test set\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "    '''\n",
    "\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    # Validation - No gradient tracking needed\n",
    "    with torch.no_grad():\n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        for j, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            #loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            #test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        \n",
    "            print(acc)\n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            test_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "            \n",
    "    # Find average test loss and test accuracy\n",
    "    #avg_test_loss = test_loss/data_size\n",
    "    avg_test_acc = test_acc/data_size\n",
    "    return avg_test_acc\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "loss_func = nn.CrossEntropyLoss() #for a multi-class classification problem \n",
    "\n",
    "model = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "model.fc = nn.Linear(1024, 2)\n",
    "\n",
    "model_file = 'models/mobilenet_v3_small_1_Linear_25e_20000_7000.pt'\n",
    "if os.path.exists(model_file):    \n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model = model.to(device)\n",
    "    avg_test_acc=computeTestSetAccuracy(model, loss_func, data_loaders[\"test\"][\"dl\"], data_loaders[\"test\"][\"size\"])\n",
    "    print(\"Test accuracy : \" + str(avg_test_acc))\n",
    "else:\n",
    "    print(\"Warrning: No Pytorch model for classification: resnet152.pt. Please Download it from GitHub link.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
